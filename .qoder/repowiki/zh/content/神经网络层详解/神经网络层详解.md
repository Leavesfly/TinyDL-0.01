# 神经网络层详解

<cite>
**本文档中引用的文件**  
- [AffineLayer.java](file://src/main/java/io/leavesfly/tinydl/nnet/layer/dnn/AffineLayer.java)
- [LinearLayer.java](file://src/main/java/io/leavesfly/tinydl/nnet/layer/dnn/LinearLayer.java)
- [ReLuLayer.java](file://src/main/java/io/leavesfly/tinydl/nnet/layer/activate/ReLuLayer.java)
- [SigmoidLayer.java](file://src/main/java/io/leavesfly/tinydl/nnet/layer/activate/SigmoidLayer.java)
- [TanhLayer.java](file://src/main/java/io/leavesfly/tinydl/nnet/layer/activate/TanhLayer.java)
- [Im2ColUtil.java](file://src/main/java/io/leavesfly/tinydl/nnet/layer/cnn/Im2ColUtil.java)
- [ConvLayer.java](file://src/main/java/io/leavesfly/tinydl/nnet/layer/cnn/ConvLayer.java)
- [PoolingLayer.java](file://src/main/java/io/leavesfly/tinydl/nnet/layer/cnn/PoolingLayer.java)
- [SimpleRnnlayer.java](file://src/main/java/io/leavesfly/tinydl/nnet/layer/rnn/SimpleRnnlayer.java)
- [LstmLayer.java](file://src/main/java/io/leavesfly/tinydl/nnet/layer/rnn/LstmLayer.java)
- [Transformer.java](file://src/main/java/io/leavesfly/tinydl/nnet/block/transformer/Transformer.java)
</cite>

## 目录
1. [全连接层](#全连接层)
2. [激活函数层](#激活函数层)
3. [卷积神经网络层](#卷积神经网络层)
4. [循环神经网络层](#循环神经网络层)
5. [Transformer模块](#transformer模块)

## 全连接层

全连接层（Fully Connected Layer）是深度神经网络中最基础的构建单元，其核心思想是将输入数据与权重矩阵进行线性变换，并可选择性地加上偏置项。在本框架中，`LinearLayer` 和 `AffineLayer` 均实现了全连接操作，二者功能相似，主要区别在于构造方式和输入形状的处理。

### LinearLayer 与 AffineLayer 实现原理

`LinearLayer` 和 `AffineLayer` 都继承自 `Layer` 类，通过 `layerForward` 方法实现前向传播。其核心计算为线性变换：  
$$
\mathbf{y} = \mathbf{x} \cdot \mathbf{W} + \mathbf{b}
$$  
其中 $\mathbf{x}$ 为输入，$\mathbf{W}$ 为权重矩阵，$\mathbf{b}$ 为偏置向量（可选）。

- **权重初始化**：采用 Xavier 初始化策略，即从均值为0、标准差为 $\sqrt{1 / d_{in}}$ 的正态分布中采样，其中 $d_{in}$ 为输入维度，有助于缓解梯度消失问题。
- **前向传播**：调用 `Variable.linear()` 方法完成矩阵乘法与偏置加法。
- **反向传播**：梯度通过自动微分机制自动计算，无需在层内显式实现。

**Section sources**
- [LinearLayer.java](file://src/main/java/io/leavesfly/tinydl/nnet/layer/dnn/LinearLayer.java#L1-L54)
- [AffineLayer.java](file://src/main/java/io/leavesfly/tinydl/nnet/layer/dnn/AffineLayer.java#L1-L53)

## 激活函数层

激活函数层引入非线性变换，使神经网络能够拟合复杂函数。本框架实现了 ReLU、Sigmoid 和 Tanh 三种常用激活函数。

### ReLuLayer

ReLU（Rectified Linear Unit）函数定义为：  
$$
f(x) = \max(0, x)
$$  
其导数为：  
$$
f'(x) = 
\begin{cases}
1, & x > 0 \\
0, & x \leq 0
\end{cases}
$$  
ReLU 计算简单且能有效缓解梯度消失，是现代神经网络中最常用的激活函数。

**Section sources**
- [ReLuLayer.java](file://src/main/java/io/leavesfly/tinydl/nnet/layer/activate/ReLuLayer.java#L1-L52)

### SigmoidLayer

Sigmoid 函数定义为：  
$$
f(x) = \frac{1}{1 + e^{-x}}
$$  
其导数为：  
$$
f'(x) = f(x)(1 - f(x))
$$  
Sigmoid 将输出压缩至 (0,1) 区间，常用于二分类问题的输出层，但易导致梯度消失。

**Section sources**
- [SigmoidLayer.java](file://src/main/java/io/leavesfly/tinydl/nnet/layer/activate/SigmoidLayer.java#L1-L43)

### TanhLayer

Tanh 函数定义为：  
$$
f(x) = \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
$$  
其导数为：  
$$
f'(x) = 1 - \tanh^2(x)
$$  
Tanh 输出范围为 (-1,1)，均值为0，相比 Sigmoid 更有利于训练。

**Section sources**
- [TanhLayer.java](file://src/main/java/io/leavesfly/tinydl/nnet/layer/activate/TanhLayer.java#L1-L43)

## 卷积神经网络层

卷积层（ConvLayer）通过滑动滤波器提取局部特征，是计算机视觉任务的核心组件。

### ConvLayer 实现原理

`ConvLayer` 利用 `Im2ColUtil` 工具类将卷积操作转换为矩阵乘法以提升效率。具体流程如下：

1. **Im2Col 转换**：将输入张量（形状为 `[N, C, H, W]`）通过 `im2col` 方法展开为二维矩阵，每一行对应一个卷积窗口的展开。
2. **矩阵乘法**：将展开后的输入矩阵与展平的滤波器权重矩阵相乘，得到输出。
3. **Reshape 与 Transpose**：将输出矩阵重塑并转置为 `[N, Out_C, Out_H, Out_W]` 形状。

该方法将复杂的多维卷积简化为标准的 GEMM（GEneral Matrix Multiplication）操作，便于优化和加速。

**Section sources**
- [ConvLayer.java](file://src/main/java/io/leavesfly/tinydl/nnet/layer/cnn/ConvLayer.java#L1-L130)
- [Im2ColUtil.java](file://src/main/java/io/leavesfly/tinydl/nnet/layer/cnn/Im2ColUtil.java#L1-L69)

### PoolingLayer

池化层用于下采样，减少特征图尺寸，增强模型的平移不变性。常见的有最大池化（Max Pooling）和平均池化（Average Pooling）。其实现同样依赖 `Im2ColUtil` 进行窗口展开，然后在每个窗口内进行聚合操作（如取最大值或平均值）。

**Section sources**
- [PoolingLayer.java](file://src/main/java/io/leavesfly/tinydl/nnet/layer/cnn/PoolingLayer.java#L1-L85)

## 循环神经网络层

RNN 层用于处理序列数据，具有记忆能力。

### SimpleRnnlayer

`SimpleRnnlayer` 实现了基础的循环神经网络单元。其隐藏状态更新公式为：  
$$
\mathbf{h}_t = \tanh(\mathbf{W}_{xh} \mathbf{x}_t + \mathbf{W}_{hh} \mathbf{h}_{t-1} + \mathbf{b})
$$  
其中 $\mathbf{W}_{xh}$ 为输入到隐藏的权重，$\mathbf{W}_{hh}$ 为隐藏到隐藏的权重，$\mathbf{b}$ 为偏置。

**Section sources**
- [SimpleRnnlayer.java](file://src/main/java/io/leavesfly/tinydl/nnet/layer/rnn/SimpleRnnlayer.java#L1-L85)

### LstmLayer

LSTM（Long Short-Term Memory）通过门控机制解决长序列训练中的梯度问题。其包含三个门：遗忘门 $f_t$、输入门 $i_t$、输出门 $o_t$，以及候选记忆单元 $\tilde{C}_t$。更新公式如下：

$$
\begin{aligned}
f_t &= \sigma(\mathbf{W}_{xf} \mathbf{x}_t + \mathbf{W}_{hf} \mathbf{h}_{t-1} + \mathbf{b}_f) \\
i_t &= \sigma(\mathbf{W}_{xi} \mathbf{x}_t + \mathbf{W}_{hi} \mathbf{h}_{t-1} + \mathbf{b}_i) \\
\tilde{C}_t &= \tanh(\mathbf{W}_{xu} \mathbf{x}_t + \mathbf{W}_{hu} \mathbf{h}_{t-1} + \mathbf{b}_u) \\
C_t &= f_t \odot C_{t-1} + i_t \odot \tilde{C}_t \\
o_t &= \sigma(\mathbf{W}_{xo} \mathbf{x}_t + \mathbf{W}_{ho} \mathbf{h}_{t-1} + \mathbf{b}_o) \\
\mathbf{h}_t &= o_t \odot \tanh(C_t)
\end{aligned}
$$

**Section sources**
- [LstmLayer.java](file://src/main/java/io/leavesfly/tinydl/nnet/layer/rnn/LstmLayer.java#L1-L152)

## Transformer模块

Transformer 是一种基于自注意力机制的复杂网络结构，广泛应用于自然语言处理。

### Transformer 组成

`Transformer` 类由 `Encoder` 和 `Decoder` 两个子模块构成：

- **Encoder**：包含多头自注意力机制（Multi-Head Attention）、前馈神经网络（Feed-Forward）、残差连接和层归一化（Layer Normalization）。
- **Decoder**：结构类似 Encoder，但额外包含编码器-解码器注意力层，用于关注编码器的输出。
- **输入处理**：通常需要词嵌入（Embedding）和位置编码（Positional Encoding）。

`Transformer` 的 `layerForward` 方法首先通过编码器处理输入序列，然后将编码状态传递给解码器进行解码。

**Section sources**
- [Transformer.java](file://src/main/java/io/leavesfly/tinydl/nnet/block/transformer/Transformer.java#L1-L48)